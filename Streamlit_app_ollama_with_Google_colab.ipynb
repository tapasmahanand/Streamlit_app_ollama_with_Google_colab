{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tapasmahanand/Streamlit_app_ollama_with_Google_colab/blob/main/Streamlit_app_ollama_with_Google_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApKoENkyzqay"
      },
      "outputs": [],
      "source": [
        "# Cell 1 - Install Required Libraries:\n",
        "!pip install streamlit PyMuPDF4LLM pandas openpyxl pyngrok requests  # Added 'requests' for Ollama API calls\n",
        "\n",
        "# Install Ollama (using a convenient installation script)\n",
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "\n",
        "# Install and setup ngrok (as in your original code)\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null && echo \"deb https://ngrok-agent.s3.amazonaws.com buster main\" | sudo tee /etc/apt/sources.list.d/ngrok.list && sudo apt update && sudo apt install ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "thcD8-3Ezqdy"
      },
      "outputs": [],
      "source": [
        "# Cell 2 - Import Libraries and Set Up Environment:\n",
        "import streamlit as st\n",
        "import fitz  # PyMuPDF4LLM\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import requests  # For making requests to the Ollama API\n",
        "from google.colab import files\n",
        "\n",
        "from google.colab import userdata\n",
        "# Remove Groq API key setup\n",
        "# os.environ[\"GROQ_API_KEY\"] = userdata.get('groq_colab_key')\n",
        "os.environ[\"NGROK_AUTH_TOKEN\"] = userdata.get('NGROK_AUTH_TOKEN')\n",
        "\n",
        "# Get ngrok auth token from environment\n",
        "ngrok_token = userdata.get('NGROK_AUTH_TOKEN')\n",
        "!ngrok authtoken {ngrok_token}\n",
        "\n",
        "# Start Ollama in the background (replace 'model_one' and 'model_two' with your actual model names)\n",
        "!nohup ollama serve > ollama.log 2>&1 &\n",
        "!ollama pull gemma2  # 9B Model\n",
        "!ollama pull llama3.2 # 3B Model\n",
        "time.sleep(10) # Give Ollama time to start"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwmD2W7R1IZp",
        "outputId": "75cd982a-d9b2-4ebe-c137-7ab756edfd08"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME               ID              SIZE      MODIFIED       \n",
            "llama3.2:latest    a80c4f17acd5    2.0 GB    10 seconds ago    \n",
            "gemma2:latest      ff02c3702f32    5.4 GB    39 seconds ago    \n"
          ]
        }
      ],
      "source": [
        "!ollama list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJVEpDfxzqgg",
        "outputId": "f611555c-26e5-45df-8b61-82fb69d0eef1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import fitz\n",
        "import pandas as pd\n",
        "import os\n",
        "import time\n",
        "import requests  # Changed from 'from groq import Groq'\n",
        "import tempfile\n",
        "import io\n",
        "\n",
        "# Ollama API endpoint\n",
        "OLLAMA_API_URL = \"http://localhost:11434/api/generate\"\n",
        "\n",
        "# Define models configuration for Ollama\n",
        "MODEL_CONFIGS = {\n",
        "    \"model1\": {\n",
        "        \"name\": \"llama3.2\",  # Replace with your actual Ollama model name\n",
        "        \"display_name\": \"llama3.2\"\n",
        "    },\n",
        "    \"model2\": {\n",
        "        \"name\": \"gemma2\",  # Replace with your actual Ollama model name\n",
        "        \"display_name\": \"gemma2\"\n",
        "    }\n",
        "}\n",
        "\n",
        "def chunk_text(text, chunk_size=1000):\n",
        "    \"\"\"Split text into chunks of approximately chunk_size words.\"\"\"\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "    current_size = 0\n",
        "\n",
        "    for word in words:\n",
        "        current_chunk.append(word)\n",
        "        current_size += 1\n",
        "\n",
        "        if current_size >= chunk_size:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = []\n",
        "            current_size = 0\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    \"\"\"Extract text content from PDF file.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in doc:\n",
        "            text += page.get_text()\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error processing PDF {pdf_path}: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "def get_summary_from_ollama(text, model_name, max_words=None):\n",
        "    \"\"\"Get summary from Ollama API with specified model and token management.\"\"\"\n",
        "    try:\n",
        "        chunks = chunk_text(text)\n",
        "        summaries = []\n",
        "\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            time.sleep(2) # Rate limiting\n",
        "\n",
        "            if len(chunks) > 1:\n",
        "                if i == 0:\n",
        "                    prompt = f\"This is part 1 of {len(chunks)} parts. \"\n",
        "                else:\n",
        "                    prompt = f\"This is part {i+1} of {len(chunks)} parts. \"\n",
        "\n",
        "                if max_words:\n",
        "                    prompt += f\"Please provide a brief summary of this part (the full summary across all parts should be under {max_words} words):\\n\\n{chunk}\"\n",
        "                else:\n",
        "                    prompt += f\"Please provide a concise summary of this part:\\n\\n{chunk}\"\n",
        "            else:\n",
        "                if max_words:\n",
        "                    prompt = f\"Please provide a summary of the following text in under {max_words} words:\\n\\n{chunk}\"\n",
        "                else:\n",
        "                    prompt = f\"Please provide a concise summary of the following text:\\n\\n{chunk}\"\n",
        "\n",
        "            payload = {\n",
        "                \"prompt\": prompt,\n",
        "                \"model\": model_name,\n",
        "                \"stream\": False  # Set to False for a single response\n",
        "            }\n",
        "            response = requests.post(OLLAMA_API_URL, json=payload)\n",
        "            response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "            summary = response.json()['response']\n",
        "            summaries.append(summary)\n",
        "\n",
        "        if len(summaries) > 1:\n",
        "            combined_text = \" \".join(summaries)\n",
        "            time.sleep(2) # Rate limiting\n",
        "            final_prompt = f\"Please provide a {'concise' if not max_words else f'under {max_words} words'} summary combining these separate summaries:\\n\\n{combined_text}\"\n",
        "            payload = {\n",
        "                \"prompt\": final_prompt,\n",
        "                \"model\": model_name,\n",
        "                \"stream\": False\n",
        "            }\n",
        "            final_response = requests.post(OLLAMA_API_URL, json=payload)\n",
        "            final_response.raise_for_status()\n",
        "            return final_response.json()['response']\n",
        "        else:\n",
        "            return summaries[0] if summaries else None\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        st.error(f\"Error communicating with Ollama API: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error getting summary from Ollama: {e}\")\n",
        "        return None\n",
        "\n",
        "def process_pdfs(uploaded_files, selected_model):\n",
        "    \"\"\"Process PDFs and generate summaries using the selected Ollama model.\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for uploaded_file in uploaded_files:\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "            tmp_file.write(uploaded_file.getvalue())\n",
        "            tmp_path = tmp_file.name\n",
        "\n",
        "        text = extract_text_from_pdf(tmp_path)\n",
        "        os.unlink(tmp_path)  # Clean up temporary file\n",
        "\n",
        "        if text:\n",
        "            result = {'PDF_Name': uploaded_file.name}\n",
        "\n",
        "            # Generate summaries for the selected model\n",
        "            model_config = next(item for item in MODEL_CONFIGS.values() if item['display_name'] == selected_model)\n",
        "            model_name = model_config['name']\n",
        "            display_name = model_config['display_name']\n",
        "\n",
        "            # Get 50-word summary\n",
        "            summary_50 = get_summary_from_ollama(text, model_name, max_words=50)\n",
        "            result[f'{display_name}_50_Words'] = summary_50\n",
        "\n",
        "            # Get unlimited summary\n",
        "            summary_unlimited = get_summary_from_ollama(text, model_name)\n",
        "            result[f'{display_name}_Unlimited'] = summary_unlimited\n",
        "\n",
        "            results.append(result)\n",
        "\n",
        "    return results\n",
        "\n",
        "def export_to_excel(df):\n",
        "    \"\"\"Export DataFrame to Excel file in memory.\"\"\"\n",
        "    output = io.BytesIO()\n",
        "    with pd.ExcelWriter(output, engine='openpyxl') as writer:\n",
        "        df.to_excel(writer, index=False)\n",
        "    output.seek(0)\n",
        "    return output\n",
        "\n",
        "def main():\n",
        "    st.title(\"LLM PDF Summarization\")\n",
        "\n",
        "    # Model selection in sidebar\n",
        "    st.sidebar.title(\"Select LLM Model\")\n",
        "    model_options = [config['display_name'] for config in MODEL_CONFIGS.values()]\n",
        "    selected_model = st.sidebar.selectbox(\"Choose a model\", model_options)\n",
        "\n",
        "    # File uploader for PDFs\n",
        "    uploaded_files = st.file_uploader(\"Upload PDF files\", type=['pdf'], accept_multiple_files=True)\n",
        "\n",
        "    if uploaded_files:\n",
        "        if st.button(\"Generate Summaries\"):\n",
        "            with st.spinner(f\"Processing PDFs with {selected_model}...\"):\n",
        "                results = process_pdfs(uploaded_files, selected_model)\n",
        "\n",
        "                # Display results\n",
        "                for result in results:\n",
        "                    st.subheader(f\"Results for {result['PDF_Name']}\")\n",
        "                    display_name = selected_model\n",
        "                    st.write(f\"{display_name} Summaries:\")\n",
        "                    st.write(\"50 Words:\", result[f'{display_name}_50_Words'])\n",
        "                    st.write(\"Unlimited:\", result[f'{display_name}_Unlimited'])\n",
        "\n",
        "                # Create Excel file and provide download button\n",
        "                df = pd.DataFrame(results)\n",
        "                excel_file = export_to_excel(df)\n",
        "                st.download_button(\n",
        "                    label=\"Download Excel Report\",\n",
        "                    data=excel_file,\n",
        "                    file_name=\"summary_report.xlsx\",\n",
        "                    mime=\"application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\"\n",
        "                )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI5pS1UCzqjE",
        "outputId": "a0e86827-9ee8-4123-c1dd-33eeedab8088"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app URL: https://bb81-34-142-192-207.ngrok-free.app\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing Streamlit processes\n",
        "!kill -9 $(pgrep streamlit) 2>/dev/null\n",
        "\n",
        "# Start Streamlit\n",
        "!streamlit run app.py &>/content/logs.txt &\n",
        "time.sleep(5)\n",
        "\n",
        "# Create ngrok tunnel with correct configuration\n",
        "ngrok_tunnel = ngrok.connect(addr=\"8501\", proto=\"http\", bind_tls=True)\n",
        "print(f\"Streamlit app URL: {ngrok_tunnel.public_url}\")\n",
        "\n",
        "# Keep the tunnel open\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Closing ngrok tunnel...\")\n",
        "    ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}